# References

<!-- Generate with
bibtex2html -dl -nobibsource -noheader -nofooter -noabstract -nokeywords -d -nodoc -s apalike hexapod.bib
-->

<dl>

<dt>
[<a name="kingma2014method">Kingma and Ba, 2014</a>]
</dt>
<dd>
Kingma, D.&nbsp;P. and Ba, J. (2014).
 Adam: A method for stochastic optimization.
[&nbsp;<a href="http://arxiv.org/abs/1412.6980">http</a>&nbsp;]

</dd>


<dt>
[<a name="coumans2020">Coumans and Bai, 2016</a>]
</dt>
<dd>
Coumans, E. and Bai, Y. (2016).
 Pybullet, a python module for physics simulation for games, robotics
  and machine learning.
[&nbsp;<a href="http://pybullet.org">http</a>&nbsp;]

</dd>


<dt>
[<a name="brockman2016openai">Brockman et&nbsp;al., 2016</a>]
</dt>
<dd>
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W. (2016).
 Openai gym.
[&nbsp;<a href="http://arxiv.org/abs/1606.01540">http</a>&nbsp;]

</dd>


<dt>
[<a name="DuanCHSA16">Duan et&nbsp;al., 2016</a>]
</dt>
<dd>
Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P. (2016).
 Benchmarking deep reinforcement learning for continuous control.
 <em>CoRR</em>.
[&nbsp;<a href="http://arxiv.org/abs/1604.06778">http</a>&nbsp;]

</dd>


<dt>
[<a name="henderson2017">Henderson et&nbsp;al., 2017</a>]
</dt>
<dd>
Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D.
  (2017).
 Deep reinforcement learning that matters.
[&nbsp;<a href="http://arxiv.org/abs/1709.06560">http</a>&nbsp;]

</dd>


<dt>
[<a name="schulman2017ppo">Schulman et&nbsp;al., 2017</a>]
</dt>
<dd>
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017).
 Proximal policy optimization algorithms.
 <em>CoRR</em>.
[&nbsp;<a href="http://arxiv.org/abs/1707.06347">http</a>&nbsp;]

</dd>


<dt>
[<a name="SpinningUp2018">Achiam, 2018</a>]
</dt>
<dd>
Achiam, J. (2018).
 Spinning up in deep reinforcement learning.
[&nbsp;<a href="https://spinningup.openai.com">http</a>&nbsp;]

</dd>
</dl>
