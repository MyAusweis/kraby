@misc{coumans2020,
	author = {Coumans, Erwin and Bai, Yunfei},
	title = {PyBullet, a Python module for physics simulation for games, robotics and machine learning},
	url = {http://pybullet.org},
	year = {2016}
}

@misc{henderson2017,
	abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
	added-at = {2018-10-11T20:16:13.000+0200},
	author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
	description = {[1709.06560] Deep Reinforcement Learning that Matters},
	interhash = {6f4ef32093f8db0b16430338bda8a326},
	intrahash = {b5bd5f75948f959eac4a2bf2fce9ef42},
	keywords = {reinforcement-learning deep-learning paper 2017 arxiv},
	timestamp = {2018-10-11T20:16:13.000+0200},
	title = {Deep Reinforcement Learning that Matters},
	url = {https://arxiv.org/abs/1709.06560},
	x-fetchedfrom = {Bibsonomy},
	year = 2017
}

@article{SpinningUp2018,
	author = {Achiam, Joshua},
	title = {Spinning Up in Deep Reinforcement Learning},
	url = {https://spinningup.openai.com},
	year = {2018}
}

@article{schulman2017ppo,
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time. },
	added-at = {2019-12-16T18:31:56.000+0100},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	interhash = {f57ff463a90dbafb77d55a25aea8355c},
	intrahash = {4bbcce6aa1c42ae7f61ef8cf5475aa85},
	journal = {CoRR},
	keywords = {reinforcement\_learning DRLAlgoComparison ppo},
	timestamp = {2019-12-18T21:15:59.000+0100},
	title = {Proximal Policy Optimization Algorithms.},
	url = {https://arxiv.org/abs/1707.06347},
	x-fetchedfrom = {Bibsonomy},
	year = 2017
}

@misc{brockman2016openai,
	abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
	added-at = {2018-04-12T12:08:39.000+0200},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	description = {[1606.01540] OpenAI Gym},
	interhash = {cfd0ba0b44eda9a3ca67480dfbf823a0},
	intrahash = {cdc8f927d6c8657ea82951a09e34161a},
	keywords = {reinforcement-learning paper 2016 arxiv},
	timestamp = {2018-04-12T12:08:39.000+0200},
	title = {OpenAI Gym},
	url = {https://arxiv.org/abs/1606.01540},
	x-fetchedfrom = {Bibsonomy},
	year = 2016
}

@article{DuanCHSA16,
	added-at = {2018-09-03T00:00:00.000+0200},
	author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
	interhash = {db2637d1c7b04c00569ce87e5d4f44de},
	intrahash = {4c781ed5ea10acbe25a58dd48dfd2940},
	journal = {CoRR},
	timestamp = {2018-09-04T11:38:39.000+0200},
	title = {Benchmarking Deep Reinforcement Learning for Continuous Control.},
	url = {https://arxiv.org/abs/1604.06778},
	x-fetchedfrom = {Bibsonomy},
	year = 2016
}

@misc{kingma2014method,
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	added-at = {2019-04-05T18:25:45.000+0200},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	description = {Adam: A Method for Stochastic Optimization},
	interhash = {57d2ac873f398f21bb94790081e80394},
	intrahash = {d53bcfff0fe1a1d3a4a171352ee6e92c},
	keywords = {machinelearn},
	timestamp = {2019-04-05T18:25:45.000+0200},
	title = {Adam: A Method for Stochastic Optimization},
	url = {https://arxiv.org/abs/1412.6980},
	x-fetchedfrom = {Bibsonomy},
	year = 2014
}

@article{schulman2015high,
	author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
	title = {High-dimensional continuous control using generalized advantage estimation},
	url = {https://arxiv.org/abs/1506.02438},
	x-fetchedfrom = {Google Scholar},
	year = {2015}
}

@article{tan2018sim,
	author = {Tan, Jie and Zhang, Tingnan and Coumans, Erwin and Iscen, Atil and Bai, Yunfei and Hafner, Danijar and Bohez, Steven and Vanhoucke, Vincent},
	title = {Sim-to-real: Learning agile locomotion for quadruped robots},
	url = {https://arxiv.org/abs/1804.10332},
	x-fetchedfrom = {Google Scholar},
	year = {2018}
}

@book{sutton1998,
	abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In <i>Reinforcement Learning</i>, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.<br /> <br /> The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
	added-at = {2009-10-23T10:49:34.000+0200},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	description = {phd thesis version 2009-10-23},
	howpublished = {Hardcover},
	interhash = {9369ad4b6ab9e5aa15fb456143e9a09d},
	intrahash = {22063d43301bdff5387f812f3d59499b},
	isbn = {0262193981},
	keywords = {imported},
	month = mar,
	publisher = {The MIT Press},
	timestamp = {2009-10-23T10:49:38.000+0200},
	title = {Reinforcement Learning: An Introduction},
	x-fetchedfrom = {Bibsonomy},
	year = 1998
}

@techreport{madgwick2010efficient,
	added-at = {2011-02-06T21:03:18.000+0100},
	author = {Madgwick, S.O.H. and Vaidyanathan, R. and Harrison, A.J.L.},
	biburl = {https://www.bibsonomy.org/bibtex/29e4848a7c0f71511eaa0d7ba00a658ce/kw},
	institution = {Department of Mechanical Engineering},
	interhash = {e5a45a40c4dde5cce866fb79097f741b},
	intrahash = {9e4848a7c0f71511eaa0d7ba00a658ce},
	keywords = {orientation mybsc filter},
	month = apr,
	school = {University of Bristo},
	timestamp = {2011-02-06T21:03:18.000+0100},
	title = {An Efficient Orientation Filter for Inertial Measurement Units (IMUs) and Magnetic Angular Rate and Gravity (MARG) Sensor Arrays},
	url = {http://www.scribd.com/doc/29754518/A-Efficient-Orientation-Filter-for-IMUs-and-MARG-Sensor-Arrays},
	x-fetchedfrom = {Bibsonomy},
	year = 2010
}

@misc{stable-baselines,
	author = {Hill, Ashley and Raffin, Antonin and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Traore, Rene and Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
	publisher = {GitHub},
	title = {Stable Baselines},
	url = {https://github.com/hill-a/stable-baselines},
	year = {2018}
}

@misc{rl-zoo,
	author = {Raffin, Antonin},
	publisher = {GitHub},
	title = {RL Baselines Zoo},
	url = {https://github.com/araffin/rl-baselines-zoo},
	year = {2018}
}

@article{journals/corr/abs-1907-10902,
	added-at = {2019-08-01T00:00:00.000+0200},
	author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
	interhash = {577dfef81e0a7f9fb79e3e4246906f15},
	intrahash = {544ef347f5c3ff1af99fe16282239b7b},
	journal = {CoRR},
	timestamp = {2019-08-02T11:35:09.000+0200},
	title = {Optuna: A Next-generation Hyperparameter Optimization Framework.},
	url = {http://arxiv.org/abs/1907.10902},
	x-fetchedfrom = {Bibsonomy},
	year = 2019
}

