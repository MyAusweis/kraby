@misc{coumans2020,
	author = {Coumans, Erwin and Bai, Yunfei},
	title = {PyBullet, a Python module for physics simulation for games, robotics and machine learning},
	url = {http://pybullet.org},
	year = {2016}
}

@misc{henderson2017,
	abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
	added-at = {2018-10-11T20:16:13.000+0200},
	author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
	description = {[1709.06560] Deep Reinforcement Learning that Matters},
	interhash = {6f4ef32093f8db0b16430338bda8a326},
	intrahash = {b5bd5f75948f959eac4a2bf2fce9ef42},
	keywords = {reinforcement-learning deep-learning paper 2017 arxiv},
	timestamp = {2018-10-11T20:16:13.000+0200},
	title = {Deep Reinforcement Learning that Matters},
	url = {http://arxiv.org/abs/1709.06560},
	x-fetchedfrom = {Bibsonomy},
	year = 2017
}

@article{SpinningUp2018,
	author = {Achiam, Joshua},
	title = {Spinning Up in Deep Reinforcement Learning},
	url = {https://spinningup.openai.com},
	year = {2018}
}

@article{schulman2017ppo,
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time. },
	added-at = {2019-12-16T18:31:56.000+0100},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	interhash = {f57ff463a90dbafb77d55a25aea8355c},
	intrahash = {4bbcce6aa1c42ae7f61ef8cf5475aa85},
	journal = {CoRR},
	keywords = {reinforcement\_learning DRLAlgoComparison ppo},
	timestamp = {2019-12-18T21:15:59.000+0100},
	title = {Proximal Policy Optimization Algorithms.},
	url = {http://arxiv.org/abs/1707.06347},
	x-fetchedfrom = {Bibsonomy},
	year = 2017
}

@misc{brockman2016openai,
	abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
	added-at = {2018-04-12T12:08:39.000+0200},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	description = {[1606.01540] OpenAI Gym},
	interhash = {cfd0ba0b44eda9a3ca67480dfbf823a0},
	intrahash = {cdc8f927d6c8657ea82951a09e34161a},
	keywords = {reinforcement-learning paper 2016 arxiv},
	timestamp = {2018-04-12T12:08:39.000+0200},
	title = {OpenAI Gym},
	url = {http://arxiv.org/abs/1606.01540},
	x-fetchedfrom = {Bibsonomy},
	year = 2016
}

@article{DuanCHSA16,
	added-at = {2018-09-03T00:00:00.000+0200},
	author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
	interhash = {db2637d1c7b04c00569ce87e5d4f44de},
	intrahash = {4c781ed5ea10acbe25a58dd48dfd2940},
	journal = {CoRR},
	timestamp = {2018-09-04T11:38:39.000+0200},
	title = {Benchmarking Deep Reinforcement Learning for Continuous Control.},
	url = {http://arxiv.org/abs/1604.06778},
	x-fetchedfrom = {Bibsonomy},
	year = 2016
}

@misc{kingma2014method,
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	added-at = {2019-04-05T18:25:45.000+0200},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	description = {Adam: A Method for Stochastic Optimization},
	interhash = {57d2ac873f398f21bb94790081e80394},
	intrahash = {d53bcfff0fe1a1d3a4a171352ee6e92c},
	keywords = {machinelearn},
	timestamp = {2019-04-05T18:25:45.000+0200},
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	x-fetchedfrom = {Bibsonomy},
	year = 2014
}

